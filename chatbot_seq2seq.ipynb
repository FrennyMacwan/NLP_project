{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WndtgwPD5DfW"
      },
      "source": [
        "importing libraries and initializing constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtHP4Nw5FUhZ"
      },
      "source": [
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' \n",
        "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
        "\n",
        "limit = {\n",
        "        'maxq' : 25,\n",
        "        'minq' : 2,\n",
        "        'maxa' : 25,\n",
        "        'mina' : 2\n",
        "        }\n",
        "\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 8000\n",
        "\n",
        "import random\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lRsOa15KJB"
      },
      "source": [
        "Loading movie_lines.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9Ztxn5FnIw"
      },
      "source": [
        "def get_id2line():\n",
        "    lines=open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    id2line = {}\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5:\n",
        "            id2line[_line[0]] = _line[4]\n",
        "    return id2line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZAz-0Nd5P6x"
      },
      "source": [
        "Loading movie_conversations.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE4_9jHHFsKu"
      },
      "source": [
        "def get_conversations():\n",
        "    conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    convs = [ ]\n",
        "    for line in conv_lines[:-1]:\n",
        "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "        convs.append(_line.split(','))\n",
        "    return convs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3EXsQ26FxtT"
      },
      "source": [
        "def extract_conversations(convs,id2line,path=''):\n",
        "    idx = 0\n",
        "    for conv in convs:\n",
        "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
        "        for line_id in conv:\n",
        "            f_conv.write(id2line[line_id])\n",
        "            f_conv.write('\\n')\n",
        "        f_conv.close()\n",
        "        idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAaFXZu9F96q"
      },
      "source": [
        "def gather_dataset(convs, id2line):\n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        if len(conv) %2 != 0:\n",
        "            conv = conv[:-1]\n",
        "        for i in range(len(conv)):\n",
        "            if i%2 == 0:\n",
        "                questions.append(id2line[conv[i]])\n",
        "            else:\n",
        "                answers.append(id2line[conv[i]])\n",
        "\n",
        "    return questions, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vitUt3UpGCs3"
      },
      "source": [
        "def prepare_seq2seq_files(questions, answers, path='',TESTSET_SIZE = 30000):\n",
        "    \n",
        "    # open files\n",
        "    train_enc = open(path + 'train.enc','w')\n",
        "    train_dec = open(path + 'train.dec','w')\n",
        "    test_enc  = open(path + 'test.enc', 'w')\n",
        "    test_dec  = open(path + 'test.dec', 'w')\n",
        "\n",
        "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        if i in test_ids:\n",
        "            test_enc.write(questions[i]+'\\n')\n",
        "            test_dec.write(answers[i]+ '\\n' )\n",
        "        else:\n",
        "            train_enc.write(questions[i]+'\\n')\n",
        "            train_dec.write(answers[i]+ '\\n' )\n",
        "        if i%10000 == 0:\n",
        "            print('\\n>> written {} lines'.format(i))\n",
        "\n",
        "    # close files\n",
        "    train_enc.close()\n",
        "    train_dec.close()\n",
        "    test_enc.close()\n",
        "    test_dec.close()\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibVDa5x9GLNm"
      },
      "source": [
        "def filter_line(line, whitelist):\n",
        "    return ''.join([ ch for ch in line if ch in whitelist ])\n",
        "\n",
        "def filter_data(qseq, aseq):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    raw_data_len = len(qseq)\n",
        "\n",
        "    assert len(qseq) == len(aseq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
        "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
        "                filtered_q.append(qseq[i])\n",
        "                filtered_a.append(aseq[i])\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPV9Si8FGRDU"
      },
      "source": [
        "def index_(tokenized_sentences, vocab_size):\n",
        "    # get frequency distribution\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # get vocabulary of 'vocab_size' most used words\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    # index2word\n",
        "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
        "    # word2index\n",
        "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
        "    return index2word, word2index, freq_dist\n",
        "\n",
        "\n",
        "def filter_unk(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    filtered_q, filtered_a = [], []\n",
        "\n",
        "    for qline, aline in zip(qtokenized, atokenized):\n",
        "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
        "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
        "        if unk_count_a <= 2:\n",
        "            if unk_count_q > 0:\n",
        "                if unk_count_q/len(qline) > 0.2:\n",
        "                    pass\n",
        "            filtered_q.append(qline)\n",
        "            filtered_a.append(aline)\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrubjypaGUgm"
      },
      "source": [
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    \n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    \n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32) \n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "\n",
        "    for i in range(data_len):\n",
        "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
        "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
        "\n",
        "        idx_q[i] = np.array(q_indices)\n",
        "        idx_a[i] = np.array(a_indices)\n",
        "\n",
        "    return idx_q, idx_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZbMJJbXGYvb"
      },
      "source": [
        "def pad_seq(seq, lookup, maxlen):\n",
        "    indices = []\n",
        "    for word in seq:\n",
        "        if word in lookup:\n",
        "            indices.append(lookup[word])\n",
        "        else:\n",
        "            indices.append(lookup[UNK])\n",
        "    return indices + [0]*(maxlen - len(seq))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paokIgfOFAbb",
        "outputId": "a3d98c33-04c1-4420-89f7-a85bc3327a2a"
      },
      "source": [
        "def process_data():\n",
        "\n",
        "    id2line = get_id2line()\n",
        "    print('>> gathered id2line dictionary.\\n')\n",
        "    convs = get_conversations()\n",
        "    print(convs[121:125])\n",
        "    print('>> gathered conversations.\\n')\n",
        "    questions, answers = gather_dataset(convs,id2line)\n",
        "\n",
        "  \n",
        "    questions = [ line.lower() for line in questions ]\n",
        "    answers = [ line.lower() for line in answers ]\n",
        "\n",
        "    print('\\n>> Filter lines')\n",
        "    questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n",
        "    answers = [ filter_line(line, EN_WHITELIST) for line in answers ]\n",
        "\n",
        "    print('\\n>> 2nd layer of filtering')\n",
        "    qlines, alines = filter_data(questions, answers)\n",
        "\n",
        "    for q,a in zip(qlines[141:145], alines[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    print('\\n>> Segment lines into words')\n",
        "    qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in qlines ]\n",
        "    atokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in alines ]\n",
        "    print('\\n:: Sample from segmented list of words')\n",
        "\n",
        "    for q,a in zip(qtokenized[141:145], atokenized[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    print('\\n >> Index words')\n",
        "    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n",
        "    \n",
        "    print('\\n >> Filter Unknowns')\n",
        "    qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n",
        "    print('\\n Final dataset len : ' + str(len(qtokenized)))\n",
        "\n",
        "\n",
        "    print('\\n >> Zero Padding')\n",
        "    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n",
        "\n",
        "    print('\\n >> Save numpy arrays to disk')\n",
        "    # save them\n",
        "    np.save('idx_q.npy', idx_q)\n",
        "    np.save('idx_a.npy', idx_a)\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "            'limit' : limit,\n",
        "            'freq_dist' : freq_dist\n",
        "                }\n",
        "\n",
        "    with open('metadata.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    unk_count = (idx_q == 1).sum() + (idx_a == 1).sum()\n",
        "    word_count = (idx_q > 1).sum() + (idx_a > 1).sum()\n",
        "\n",
        "    print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
        "    print('Dataset count : ' + str(idx_q.shape[0]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_data()\n",
        "\n",
        "\n",
        "def load_data(PATH=''):\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    idx_q = np.load(PATH + 'idx_q.npy')\n",
        "    idx_a = np.load(PATH + 'idx_a.npy')\n",
        "    return metadata, idx_q, idx_a\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> gathered id2line dictionary.\n",
            "\n",
            "[['L447', 'L448'], ['L490', 'L491'], ['L716', 'L717', 'L718', 'L719', 'L720', 'L721'], ['L750', 'L751', 'L752', 'L753', 'L754', 'L755']]\n",
            ">> gathered conversations.\n",
            "\n",
            "\n",
            ">> Filter lines\n",
            "\n",
            ">> 2nd layer of filtering\n",
            "28% filtered from original data\n",
            "q : [you hate me dont you]; a : [i dont really think you warrant that strong an emotion]\n",
            "q : [then say youll spend dollar night at the track with me]; a : [and why would i do that]\n",
            "q : [come on  the ponies the flat beer you with money in your eyes me with my hand on your ass]; a : [you  covered in my vomit]\n",
            "q : [are you following me]; a : [i was in the laundromat i saw your car thought id say hi]\n",
            "\n",
            ">> Segment lines into words\n",
            "\n",
            ":: Sample from segmented list of words\n",
            "q : [['you', 'hate', 'me', 'dont', 'you']]; a : [['i', 'dont', 'really', 'think', 'you', 'warrant', 'that', 'strong', 'an', 'emotion']]\n",
            "q : [['then', 'say', 'youll', 'spend', 'dollar', 'night', 'at', 'the', 'track', 'with', 'me']]; a : [['and', 'why', 'would', 'i', 'do', 'that']]\n",
            "q : [['come', 'on', 'the', 'ponies', 'the', 'flat', 'beer', 'you', 'with', 'money', 'in', 'your', 'eyes', 'me', 'with', 'my', 'hand', 'on', 'your', 'ass']]; a : [['you', 'covered', 'in', 'my', 'vomit']]\n",
            "q : [['are', 'you', 'following', 'me']]; a : [['i', 'was', 'in', 'the', 'laundromat', 'i', 'saw', 'your', 'car', 'thought', 'id', 'say', 'hi']]\n",
            "\n",
            " >> Index words\n",
            "\n",
            " >> Filter Unknowns\n",
            "2% filtered from original data\n",
            "\n",
            " Final dataset len : 96473\n",
            "\n",
            " >> Zero Padding\n",
            "\n",
            " >> Save numpy arrays to disk\n",
            "% unknown : 4.27073242918805\n",
            "Dataset count : 96473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShybqwoBI-8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vZoIGPA4IzwB"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from datasets.cornell_corpus import data\n",
        "import data_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJULK2bRIzwG"
      },
      "source": [
        "# load data from pickle and npy files\n",
        "metadata, idx_q, idx_a = data.load_data(PATH='datasets/cornell_corpus/')\n",
        "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E_iCnYhzIzwH"
      },
      "source": [
        "# parameters \n",
        "xseq_len = trainX.shape[-1]\n",
        "yseq_len = trainY.shape[-1]\n",
        "batch_size = 16\n",
        "xvocab_size = len(metadata['idx2w'])  \n",
        "yvocab_size = xvocab_size\n",
        "emb_dim = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AI_e-DI1IzwI"
      },
      "source": [
        "import seq2seq_wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oCtrJafIzwI",
        "outputId": "bebff9b6-532a-4e4d-bf28-a034bf2899dd"
      },
      "source": [
        "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
        "                               yseq_len=yseq_len,\n",
        "                               xvocab_size=xvocab_size,\n",
        "                               yvocab_size=yvocab_size,\n",
        "                               ckpt_path='ckpt/cornell_corpus/',\n",
        "                               emb_dim=emb_dim,\n",
        "                               num_layers=3\n",
        "                               )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<log> Building Graph </log>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "2wGblK4PIzwJ"
      },
      "source": [
        "val_batch_gen = data_utils.rand_batch_gen(validX, validY, 32)\n",
        "test_batch_gen = data_utils.rand_batch_gen(testX, testY, 256)\n",
        "train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ewQeJg1BIzwL"
      },
      "source": [
        "sess = model.restore_last_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHltJyNZIzwM",
        "outputId": "2146b724-d1a7-49ca-9ebd-8ded57caf633"
      },
      "source": [
        "input_ = test_batch_gen.__next__()[0]\n",
        "output = model.predict(sess, input_)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(256, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtljIxrNIzwN",
        "outputId": "cb85f66b-59cb-41c9-d0b5-11408f1ed14d"
      },
      "source": [
        "replies = []\n",
        "for ii, oi in zip(input_.T, output):\n",
        "    q = data_utils.decode(sequence=ii, lookup=metadata['idx2w'], separator=' ')\n",
        "    decoded = data_utils.decode(sequence=oi, lookup=metadata['idx2w'], separator=' ').split(' ')\n",
        "    if decoded.count('unk') == 0:\n",
        "        if decoded not in replies:\n",
        "            print('q : [{0}]; a : [{1}]'.format(q, ' '.join(decoded)))\n",
        "            replies.append(decoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "q : [bill id like you to meet jack torrance]; a : [how much do you think]\n",
            "q : [and who are you to talk you were nothing you couldnt even sing i must have been out of my mind]; a : [i dont know what youre talking about]\n",
            "q : [by breaking up a companys assets]; a : [what are you talking about]\n",
            "q : [what is it]; a : [i dont know]\n",
            "q : [ill see you there]; a : [ill get out]\n",
            "q : [okay ill be talking to you]; a : [youre not going to get out]\n",
            "q : [i must be outta my mind buddy quit it]; a : [okay for a minute]\n",
            "q : [when are you going to let the police know]; a : [you dont know what youre talking about]\n",
            "q : [you can do it]; a : [yeah i think so]\n",
            "q : [like hell you know if you fellows stuck together stead of letting them walk all over you they might not try it]; a : [if you werent talking about me i dont know what youre talking about]\n",
            "q : [wait are you saying that i dont appreciate]; a : [i know you know what i mean]\n",
            "q : [no you just looked as if you did]; a : [well i didnt believe you]\n",
            "q : [youre terrible]; a : [yeah but im not]\n",
            "q : [you would take a pain n the arse full stop]; a : [dont worry about it]\n",
            "q : [yeah howd the kid hold up]; a : [whats that]\n",
            "q : [yes well miss ratched feels that youre a disturbing influence on the other patients]; a : [dont you tell me]\n",
            "q : [you leading a squad]; a : [what is it]\n",
            "q : [nah i dont think so]; a : [why not]\n",
            "q : [hes pissing in our faces again and were just taking it]; a : [i cant]\n",
            "q : [my lady]; a : [she said you were a little girl]\n",
            "q : [im sorry i dont know what else to say except im sorry]; a : [why would you like to be]\n",
            "q : [mr brandon couldnt be here he might have left something for you what would it look like]; a : [thats it i think i can get it out]\n",
            "q : [nobodys gonna get you now get inside]; a : [i cant go back]\n",
            "q : [i just turned 25 i was 24 for a whole year]; a : [well i dont think so]\n",
            "q : [make sure he doesnt leave]; a : [you sure]\n",
            "q : [do we need him]; a : [i dont know what to do]\n",
            "q : [youre terry unk arent you]; a : [what are you doing here]\n",
            "q : [so lets go]; a : [i dont think so]\n",
            "q : [ya owe me twentyfive bucks]; a : [its a good man]\n",
            "q : [shall i leave]; a : [of course]\n",
            "q : [unk unk probably asleep by now do you want him to see you like this]; a : [no way i dont want to get out of here]\n",
            "q : [well i really think hes got a chance]; a : [i know]\n",
            "q : [youd better be quiet sandy]; a : [shut up]\n",
            "q : [buddy that was pretty unk of you pushing me away like that just when it was interesting]; a : [what about you]\n",
            "q : [jesus christ you scared the shit out of me]; a : [whats going on]\n",
            "q : [well im sorry im really sorry ellie]; a : [its okay]\n",
            "q : [youre not a man because of a job duff]; a : [but i said that]\n",
            "q : [he didnt lose her he threw her away]; a : [hes not dead]\n",
            "q : [you unk have the gotta]; a : [what the hell are you talking about]\n",
            "q : [whoa whoa what do you expect them to say youre alan unk]; a : [im sorry i dont know]\n",
            "q : [doc what can i tell ya]; a : [go ahead]\n",
            "q : [my lady this play will end badly i will tell]; a : [lets get out of here]\n",
            "q : [what if i said goodbye]; a : [then why are you]\n",
            "q : [im going to miss you]; a : [no youre not]\n",
            "q : [well wed love to but were going to another party]; a : [what do you mean]\n",
            "q : [dog eat dog unk you fuck other man before he fuck you and you must fuck last]; a : [what do you think we can do]\n",
            "q : [not in the trunk]; a : [what about]\n",
            "q : [can we light the candles now on the cake]; a : [you want to go home]\n",
            "q : [you look frightened have i been saying something frightening]; a : [i dont know what you mean]\n",
            "q : [you want to hear the good news first or the bad news]; a : [what the hell is that]\n",
            "q : [ya gotta be a little soft to wanna be a unk its a racket where ya almost guaranteed to end up a bum]; a : [all right]\n",
            "q : [you werent going with her]; a : [well shes not a little girl]\n",
            "q : [i dont want land]; a : [you dont have to be a good man]\n",
            "q : [what do you want from me cotton]; a : [we dont know what youre talking about]\n",
            "q : [the hopes are perfect beautiful identical smooth and they are for something really amazing i feel it in my bones]; a : [youre not a fool]\n",
            "q : [get married]; a : [what do you think]\n",
            "q : [ha dear boy i do hope this doesnt unk a meeting in private]; a : [ill get out of here]\n",
            "q : [karl you up]; a : [yeah im not going to get out of here]\n",
            "q : [i just realized this is for television isnt it i cant swear up and down like i just did]; a : [its not that you know what youre talking about]\n",
            "q : [yes george]; a : [and what do you want]\n",
            "q : [hey eddie looks like you really stepped in it this time]; a : [thats right i dont know]\n",
            "q : [you dont want to go all the way to san francisco in a unk do you i dont]; a : [im not sure]\n",
            "q : [it can happen so sudden cant it being left out on your own]; a : [its not a good idea]\n",
            "q : [what do you mean]; a : [i dont know i dont know what i mean]\n",
            "q : [i want to]; a : [you want to go]\n",
            "q : [what do you want take my wife please]; a : [well i dont know what to do]\n",
            "q : [look at this the lock is totally unk]; a : [you know what it is]\n",
            "q : [he wont steal im tellin you hes a pretty good ol boy keeps to himself]; a : [thats right]\n",
            "q : [youre really pushing it bringing me here]; a : [what do you want me to do]\n",
            "q : [im not unk with armitage and his unk breathing down my neck]; a : [you mean you dont want to go]\n",
            "q : [my god these people are insane]; a : [we dont know what they are]\n",
            "q : [no it wasnt like that]; a : [yes you did]\n",
            "q : [dont you worry about that]; a : [dont worry about me]\n",
            "q : [well i just kept unk that dish maybe it doesnt sound very sexy but it was]; a : [yeah i dont know]\n",
            "q : [hey vaughan how are you karl]; a : [im fine]\n",
            "q : [this isnt your room youre in unk i fucked up]; a : [im sorry i dont know what to do]\n",
            "q : [r was worried about you you didnt even call youre always on my case if i dont call]; a : [i know you dont know what youre talking about]\n",
            "q : [something happened you got unk in the last quarter]; a : [what do you want to do]\n",
            "q : [if youre going to be unk i wish youd be a little more discreet about it rich men like unk love you and leave you]; a : [thats not what i mean]\n",
            "q : [how long would they let me sleep]; a : [i dont know i dont know what to do]\n",
            "q : [whats got billy so unk]; a : [i dont know what to do with it]\n",
            "q : [you fucking bastard]; a : [come on]\n",
            "q : [she used to call me mr right remember that buddy]; a : [how are you going to do]\n",
            "q : [my unk in miami its nice down there]; a : [thats what i said]\n",
            "q : [speak with my lawyer]; a : [you dont know what you mean]\n",
            "q : [mr unk because you love the theatre you must have a part in my play i am writing an unk a small but vital role]; a : [thats right i dont think so]\n",
            "q : [of course you dont know anything about it if you knew anything about it i wouldnt have to send you over there to cover it]; a : [thats not what you want]\n",
            "q : [does mom know]; a : [she said she was in love]\n",
            "q : [its okay tatum shes just doing her job right gale]; a : [shes not going to see her]\n",
            "q : [wanna keep goin]; a : [im sorry]\n",
            "q : [i thought so too unk a neat guy]; a : [hey i dont know what youre doing to do with it]\n",
            "q : [did i wake you]; a : [no you dont]\n",
            "q : [how could you do this]; a : [well i dont know what i mean]\n",
            "q : [that can wait till the weekend]; a : [are you kidding]\n",
            "q : [if you cant look anymore i understand]; a : [im not going to tell you that]\n",
            "q : [maybe i can change him]; a : [then he doesnt know what he is]\n",
            "q : [maybe if i kiss him ill feel it]; a : [then you should have to]\n",
            "q : [well youre a little early]; a : [i dont know what youre doing to get out of here]\n",
            "q : [i thought michael was picking me up]; a : [dont worry about it im not going to get out of here]\n",
            "q : [i dont believe he did sir i couldnt find a single track just doesnt make sense]; a : [shut up what do you mean]\n",
            "q : [he said that]; a : [he was here]\n",
            "q : [its unk unk the name of the character she plays in the movie]; a : [i know what i mean]\n",
            "q : [hes company]; a : [hes not going to see him]\n",
            "q : [youre so proud youre like some retarded kid comin home from school look dad i got an f]; a : [thats right thats what you want to do]\n",
            "q : [feels like theres a bullet still in my chest]; a : [right for me]\n",
            "q : [not now charlie ive got a headache get used to the word roll it around your tongue for a years]; a : [its the time i dont know what to do]\n",
            "q : [and the bookstore have you been working there long]; a : [i think so]\n",
            "q : [you sent for me]; a : [yes yes i am]\n",
            "q : [damour damour why do i know that name]; a : [well i dont know]\n",
            "q : [speaking of which you run that license plate for me]; a : [i cant believe you]\n",
            "q : [i dont know unk a unk]; a : [you know what i mean]\n",
            "q : [how shes even forgot her own language]; a : [long night she was a very nice person]\n",
            "q : [just that she got away]; a : [well i think so]\n",
            "q : [anything i dont care what it is just so its something]; a : [its not that i dont know]\n",
            "q : [at this hour]; a : [no problem]\n",
            "q : [i swear it he wants romeo for ned and the unk men]; a : [and how much do you think]\n",
            "q : [oh really i thought it was pretty good]; a : [its a good idea]\n",
            "q : [look fry company says were responsible for every one of those]; a : [dont be silly]\n",
            "q : [laser unk you cant get the code wrong it unk you i cant let you try it]; a : [ill take care of you]\n",
            "q : [what if we were to put bruce into the park as a guest]; a : [dont worry ill be fine if you want to get out of here]\n",
            "q : [could be]; a : [but what do you think]\n",
            "q : [i dont know i cant say]; a : [then what did you do]\n"
          ]
        }
      ]
    }
  ]
}